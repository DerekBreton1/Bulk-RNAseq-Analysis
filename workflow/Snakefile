import pandas as pd

# Read metadata csv into a dictionary
metadata = pd.read_csv("data/SraRunTable.csv").set_index("Run").to_dict(orient="index")
# Get SRR IDs from metadata
SAMPLES = list(metadata.keys())
print(SAMPLES)
rule all:
    input:
        expand('data/{sample}.fastq.gz', sample=SAMPLES),
        'results/multiqc_report.html',
        expand('results/{sample}/abundance.h5', sample=SAMPLES)

# Download SRA files
rule sra_download:
    output:
        srr = 'data/{sample}/{sample}.sra'
    params:
        outdir = 'data/'
    conda:
        'envs/sra_toolkit_env.yaml'
    shell:
        '''
        python3 workflow/scripts/sra_download.py {wildcards.sample} {params.outdir}
        '''
# FASTQ download from SRA ID file
rule fastq_download:
    input:
        srr = 'data/{sample}/{sample}.sra'
    output:
        fastq = "data/{sample}.fastq.gz"
    params:
        outdir = 'data/'      
    conda:
        'envs/sra_toolkit_env.yaml'
    shell:
        "python3 workflow/scripts/fastq_download.py {input.srr} {params.outdir}"

# Initial quality control
rule fastqc:
    input:
        fastq = "data/{sample}.fastq.gz"
    output:
        fastqc = "results/{sample}_fastqc.html"
    params:
        outdir = 'results/'
    conda:
        'envs/fastqc_env.yaml'
    shell:
        '''
        fastqc -o {params.outdir} {input.fastq}
        '''

# Aggregate the fastqc files
rule multiqc:
    input:
        expand("results/{sample}_fastqc.html", sample=SAMPLES)
    output:
        report = 'results/multiqc_report.html'
    params:
        outdir = 'results/'
    conda:
        'envs/multiqc_env.yaml'
    shell:
        '''
        multiqc {params.outdir} -o {params.outdir}
        '''

# Download and merge cDNA and ncDNA, also download GTF. All release 113
rule download_merge_DNA:
    output:
        transcriptome = 'results/Homo_sapiens.GRCh38.rna.fa'
    params:
        cDNA = 'ftp://ftp.ensembl.org/pub/release-113/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz',
        ncDNA = 'ftp://ftp.ensembl.org/pub/release-113/fasta/homo_sapiens/ncrna/Homo_sapiens.GRCh38.ncrna.fa.gz',
        gtf = 'ftp://ftp.ensembl.org/pub/release-113/gtf/homo_sapiens/Homo_sapiens.GRCh38.113.gtf.gz'
    shell:
        '''
        wget -P data/ {params.cDNA}
        wget -P data/ {params.ncDNA}
        wget -P data/ {params.gtf}
        zcat data/Homo_sapiens.GRCh38.cdna.all.fa.gz data/Homo_sapiens.GRCh38.ncrna.fa.gz > {output.transcriptome}
        '''

# Build a Kallisto index
rule build_kallisto_index:
    input:
        transcriptome = 'results/Homo_sapiens.GRCh38.rna.fa'
    output:
        index = 'results/hsGRCh38_kallisto.idx'
    conda:
        'envs/kallisto_env.yaml'
    shell:
        '''
        kallisto index -i {output.index} {input.transcriptome}
        '''

# Quantify abundances of transcripts
rule quant:
    input:
        index = 'results/hsGRCh38_kallisto.idx',
        fastq = "data/{sample}.fastq.gz"
    output:
        abundance_h5 = 'results/{sample}/abundance.h5'
    params:
        outdir = 'results/{sample}'
    threads: 2
    conda:
        'envs/kallisto_env.yaml'
    shell:
        '''
        kallisto quant -i {input.index} -o {params.outdir} -b 50 --single -l 200 -s 20 {input.fastq}
        '''